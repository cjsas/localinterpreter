1
00:00:00,166 --> 00:00:02,102
Thank you so much, Jensen,

2
00:00:02,102 --> 00:00:03,336
for joining yet again

3
00:00:03,336 --> 00:00:05,672
for our Build developer conference.

4
00:00:05,672 --> 00:00:07,273
You were here a couple of years ago

5
00:00:07,273 --> 00:00:08,108
and we had a chance

6
00:00:08,108 --> 00:00:09,142
at that time to talk

7
00:00:09,142 --> 00:00:11,044
about all the great innovation

8
00:00:11,044 --> 00:00:13,847
you are bringing to our Azure

9
00:00:13,847 --> 00:00:15,849
and how customers were going to benefit.

10
00:00:15,849 --> 00:00:17,851
And in fact, in the last two years,

11
00:00:17,851 --> 00:00:19,786
you have sort of continued to push

12
00:00:19,786 --> 00:00:22,355
the real frontier of innovation.

13
00:00:22,355 --> 00:00:24,524
And you and I have talked a lot

14
00:00:24,524 --> 00:00:27,527
about how this may be, again, the golden age

15
00:00:27,527 --> 00:00:30,663
of both silicon hardware systems,

16
00:00:30,663 --> 00:00:33,233
software systems all coming together.

17
00:00:33,233 --> 00:00:34,067
And one of the things

18
00:00:34,067 --> 00:00:35,268
that I've really learned

19
00:00:35,268 --> 00:00:36,736
so much from you in saying, look,

20
00:00:36,736 --> 00:00:37,637
at the end of the day,

21
00:00:37,637 --> 00:00:40,607
our joint goal is to be able to deliver

22
00:00:40,607 --> 00:00:43,076
more intelligence to the world.

23
00:00:43,076 --> 00:00:44,210
In some sense you can even say

24
00:00:44,210 --> 00:00:47,247
it's tokens per dollar per watt.

25
00:00:47,247 --> 00:00:48,848
That's, sort of, what ultimately

26
00:00:48,848 --> 00:00:51,251
is going to help the world flourish.

27
00:00:51,251 --> 00:00:53,620
And, so, I just wanted to be able to, maybe,

28
00:00:53,620 --> 00:00:55,789
get your perspective on all of this.

29
00:00:55,789 --> 00:00:58,958
Maybe starting, in fact, with the very start of

30
00:00:58,958 --> 00:01:00,326
what has been our industry,

31
00:01:00,326 --> 00:01:01,361
which is Moore's Law.

32
00:01:01,361 --> 00:01:03,596
You're really taking this Moore's Law

33
00:01:03,596 --> 00:01:05,665
and put it back on hyperdrive.

34
00:01:05,665 --> 00:01:07,600
And maybe you want to speak to–

35
00:01:07,600 --> 00:01:08,701
in fact, right behind me

36
00:01:08,701 --> 00:01:11,404
is a signed GB200 from you,

37
00:01:11,404 --> 00:01:13,473
and we are now just about putting that

38
00:01:13,473 --> 00:01:15,375
into massive production. And so,

39
00:01:15,375 --> 00:01:17,377
I just want you to talk a little bit about

40
00:01:17,377 --> 00:01:19,245
how this is actually working

41
00:01:19,245 --> 00:01:21,281
generation to generation in terms of just

42
00:01:21,281 --> 00:01:23,316
the benefits of Moore's Law.

43
00:01:23,316 --> 00:01:25,085
So excited to be here.

44
00:01:25,085 --> 00:01:26,953
Satya, in fact, two years ago

45
00:01:27,320 --> 00:01:30,857
we had just launched the largest

46
00:01:30,857 --> 00:01:33,393
AI supercomputer in the world together

47
00:01:33,960 --> 00:01:35,128
on Azure.

48
00:01:35,128 --> 00:01:37,597
And then right now, as we speak,

49
00:01:37,597 --> 00:01:38,932
we are in full production

50
00:01:38,932 --> 00:01:40,800
with Grace Blackwell.

51
00:01:40,800 --> 00:01:43,136
We are ramping and scaling

52
00:01:43,136 --> 00:01:44,404
and building the largest

53
00:01:44,404 --> 00:01:47,006
AI supercomputer in the world in Azure.

54
00:01:47,307 --> 00:01:50,009
And so this is just an exciting, exciting time

55
00:01:50,009 --> 00:01:51,945
literally two years later.

56
00:01:51,945 --> 00:01:53,680
Well, during that time

57
00:01:53,680 --> 00:01:56,649
we've innovated across the entire stack.

58
00:01:56,649 --> 00:01:59,018
This is the big change

59
00:01:59,018 --> 00:02:00,987
in how computing works now.

60
00:02:00,987 --> 00:02:02,622
Instead of just the processor

61
00:02:02,622 --> 00:02:05,058
running static software, if you will,

62
00:02:05,058 --> 00:02:07,594
everything about the stack has changed.

63
00:02:07,594 --> 00:02:10,230
And so, our processor has changed,

64
00:02:10,230 --> 00:02:12,265
our NVLink has changed

65
00:02:12,265 --> 00:02:13,500
so that we can scale up

66
00:02:13,500 --> 00:02:16,669
to a much larger compute node,

67
00:02:16,669 --> 00:02:18,371
it's liquid cooled,

68
00:02:18,371 --> 00:02:21,908
the architecture is FP4 Tensor Core,

69
00:02:21,908 --> 00:02:24,110
we now have the ability to connect

70
00:02:24,110 --> 00:02:27,313
Grace and Blackwell

71
00:02:27,313 --> 00:02:30,049
coherently over a super fast link.

72
00:02:30,049 --> 00:02:32,352
Very important in the world of

73
00:02:32,352 --> 00:02:35,421
KV caching as we talk about these large

74
00:02:35,421 --> 00:02:39,225
agentic model and agentic workloads.

75
00:02:39,225 --> 00:02:40,426
And so all of this stuff

76
00:02:40,426 --> 00:02:42,729
combined with our new CUDA

77
00:02:42,729 --> 00:02:45,798
algorithms and new model technology

78
00:02:45,798 --> 00:02:48,735
in your new AI infrastructure on Azure,

79
00:02:48,735 --> 00:02:51,704
together, all of that together,

80
00:02:51,704 --> 00:02:54,240
40x speed-up,

81
00:02:54,240 --> 00:02:55,742
over Hopper.

82
00:02:55,742 --> 00:02:57,610
40x speed-up over Hopper.

83
00:02:57,610 --> 00:02:58,678
That's just an insane

84
00:02:58,678 --> 00:03:00,680
speed-up in just two years.

85
00:03:00,680 --> 00:03:02,015
It's just unbelievable.

86
00:03:02,015 --> 00:03:03,516
And in fact, you know,

87
00:03:03,516 --> 00:03:06,686
that type of compounding of S-curves,

88
00:03:06,686 --> 00:03:08,922
your innovation, our innovation

89
00:03:08,922 --> 00:03:10,924
coming together, ultimately driving

90
00:03:10,924 --> 00:03:13,560
that frontier forward is just unbelievable.

91
00:03:13,560 --> 00:03:14,694
But one of the other things

92
00:03:14,694 --> 00:03:15,361
you taught me,

93
00:03:15,361 --> 00:03:16,529
which I'll always remember,

94
00:03:16,529 --> 00:03:17,597
is that speed of light

95
00:03:17,597 --> 00:03:19,165
execution between even

96
00:03:19,165 --> 00:03:20,967
the two organizations

97
00:03:20,967 --> 00:03:23,203
is a way to deliver that.

98
00:03:23,203 --> 00:03:25,138
Which is, there is, in fact,

99
00:03:25,138 --> 00:03:26,339
you know, in our fleet,

100
00:03:26,339 --> 00:03:27,207
as you can imagine,

101
00:03:27,207 --> 00:03:29,409
we have all the generations of NVIDIA

102
00:03:29,409 --> 00:03:32,078
and the new generations coming in.

103
00:03:32,078 --> 00:03:34,380
And, do you want to speak to a little bit of what

104
00:03:34,380 --> 00:03:35,815
Moore's Law and speed

105
00:03:35,815 --> 00:03:37,116
have to do with each other

106
00:03:37,116 --> 00:03:39,085
when it comes to driving innovation?

107
00:03:39,085 --> 00:03:42,055
This is the insane execution

108
00:03:42,055 --> 00:03:43,790
of our two organizations

109
00:03:43,790 --> 00:03:46,025
now hyperlinked, if you will.

110
00:03:46,693 --> 00:03:47,660
In fact,

111
00:03:47,660 --> 00:03:49,629
when technology is moving

112
00:03:49,629 --> 00:03:52,465
40 times per generation

113
00:03:52,865 --> 00:03:55,435
and it's 40 times every two years,

114
00:03:55,969 --> 00:03:58,871
you really want to upgrade every year.

115
00:03:58,871 --> 00:04:00,306
You don't want to wait

116
00:04:00,306 --> 00:04:01,874
every four years,

117
00:04:01,874 --> 00:04:04,043
build out a giant fleet.

118
00:04:04,043 --> 00:04:06,479
You want to build out small amounts

119
00:04:06,479 --> 00:04:08,381
every single year.

120
00:04:08,381 --> 00:04:09,482
And each year, let's say

121
00:04:09,482 --> 00:04:11,784
it’s ten times faster than the last year,

122
00:04:11,784 --> 00:04:13,586
and as a result, you,

123
00:04:13,586 --> 00:04:14,787
you know, cost average,

124
00:04:14,787 --> 00:04:17,023
if you will, or inverse to that,

125
00:04:17,023 --> 00:04:19,359
performance average up your entire fleet.

126
00:04:19,359 --> 00:04:21,494
And so you want to innovate

127
00:04:21,494 --> 00:04:23,496
and you want to integrate

128
00:04:23,496 --> 00:04:24,597
new data centers,

129
00:04:24,597 --> 00:04:26,399
new computer architectures every year.

130
00:04:26,399 --> 00:04:28,067
The complexity, of course,

131
00:04:28,067 --> 00:04:29,736
is the work that you and I do

132
00:04:29,736 --> 00:04:32,071
is not about building PCs anymore.

133
00:04:32,071 --> 00:04:35,675
It's building these giant AI factories.

134
00:04:35,675 --> 00:04:37,610
The scale is incredible.

135
00:04:37,610 --> 00:04:40,213
The technology is insanely complicated.

136
00:04:40,213 --> 00:04:43,016
And for our organizations now to be innovating

137
00:04:43,016 --> 00:04:44,984
every single year requires

138
00:04:44,984 --> 00:04:46,819
really tight integration.

139
00:04:46,819 --> 00:04:48,121
And so that's the challenge

140
00:04:48,121 --> 00:04:49,422
but the benefit’s incredible.

141
00:04:49,922 --> 00:04:51,324
Yeah, and I think that that point

142
00:04:51,324 --> 00:04:53,726
about that physics of the fleet, right?

143
00:04:53,726 --> 00:04:55,528
Continuously

144
00:04:55,528 --> 00:04:59,198
taking advantage of what comes out each year,

145
00:04:59,198 --> 00:05:00,700
and that compounding of that

146
00:05:00,700 --> 00:05:04,070
40x improvement every two years, let's say.

147
00:05:04,070 --> 00:05:05,772
And then the fleet itself

148
00:05:05,772 --> 00:05:07,106
will be there for four years,

149
00:05:07,106 --> 00:05:07,907
five years,

150
00:05:07,907 --> 00:05:11,377
and the benefits of it go all through that period.

151
00:05:11,377 --> 00:05:12,612
In fact, talking about that,

152
00:05:12,612 --> 00:05:13,546
one of the other things

153
00:05:13,546 --> 00:05:15,014
you mentioned this even in, you know,

154
00:05:15,014 --> 00:05:16,282
when you talked about the first point,

155
00:05:16,282 --> 00:05:17,850
which is the software.

156
00:05:17,850 --> 00:05:19,786
The innovation you're doing in CUDA,

157
00:05:19,786 --> 00:05:21,387
some of the system software work

158
00:05:21,387 --> 00:05:23,423
that we do end to end,

159
00:05:23,423 --> 00:05:24,991
the app, where we talked a lot about even

160
00:05:24,991 --> 00:05:27,727
the new app server for AI that we are doing.

161
00:05:27,727 --> 00:05:29,228
Even something like even just,

162
00:05:29,228 --> 00:05:30,763
you know, when I look at an application

163
00:05:30,763 --> 00:05:31,631
and its performance,

164
00:05:31,631 --> 00:05:33,232
whether it's latency, COGS,

165
00:05:33,232 --> 00:05:35,768
even the smart techniques or prompt caching,

166
00:05:35,768 --> 00:05:37,937
plus everything else that happens,

167
00:05:37,937 --> 00:05:39,505
just makes a huge difference.

168
00:05:39,505 --> 00:05:40,840
But you want to talk about

169
00:05:40,840 --> 00:05:42,408
the compounding effects

170
00:05:42,408 --> 00:05:44,577
of software that then add

171
00:05:44,577 --> 00:05:46,479
to all of what we are doing together?

172
00:05:46,479 --> 00:05:47,680
This is just incredible

173
00:05:47,680 --> 00:05:50,917
about computing architectures

174
00:05:50,917 --> 00:05:53,152
that are stable,

175
00:05:53,152 --> 00:05:54,921
that are programmable,

176
00:05:54,921 --> 00:05:56,689
have a rich ecosystem.

177
00:05:56,689 --> 00:05:58,624
So, meanwhile while we're sitting here

178
00:05:58,624 --> 00:06:00,860
executing from Pascal, to Ampere,

179
00:06:00,860 --> 00:06:03,463
to Hopper, to Blackwell,

180
00:06:03,463 --> 00:06:06,065
to you know, next generation,

181
00:06:06,065 --> 00:06:09,202
all of our software architectures are compatible.

182
00:06:09,202 --> 00:06:11,871
As a result, the rich ecosystem of tools

183
00:06:11,871 --> 00:06:12,638
and all the developers

184
00:06:12,638 --> 00:06:13,940
that are developing on it,

185
00:06:13,940 --> 00:06:15,608
they're anxious to develop on it

186
00:06:15,608 --> 00:06:17,610
because the install base is so large.

187
00:06:17,610 --> 00:06:19,879
They're anxious to develop on it

188
00:06:19,879 --> 00:06:21,848
because the ecosystem is so rich.

189
00:06:21,848 --> 00:06:23,449
And all of that

190
00:06:23,449 --> 00:06:25,551
enables them to invest deeply

191
00:06:25,551 --> 00:06:27,420
into enhancing their models

192
00:06:27,420 --> 00:06:28,421
and their algorithms

193
00:06:28,421 --> 00:06:29,355
and their runtimes

194
00:06:29,355 --> 00:06:30,990
and their orchestration layers,

195
00:06:30,990 --> 00:06:31,891
because it affects

196
00:06:31,891 --> 00:06:34,093
and benefits the entire fleet.

197
00:06:34,093 --> 00:06:35,228
And so while we're sitting here

198
00:06:35,228 --> 00:06:37,263
executing new generations

199
00:06:37,263 --> 00:06:39,098
of architectures every single year,

200
00:06:39,098 --> 00:06:39,932
we still want

201
00:06:39,932 --> 00:06:41,401
architecture compatibility.

202
00:06:41,401 --> 00:06:43,970
We want the rich ecosystem to be stable

203
00:06:43,970 --> 00:06:47,707
so that software developers’ investments,

204
00:06:47,707 --> 00:06:49,909
and all of the AI developers’ investments,

205
00:06:49,909 --> 00:06:52,512
and all your infrastructure customers’ investments,

206
00:06:52,512 --> 00:06:53,846
get to be distributed

207
00:06:53,846 --> 00:06:56,048
and amortized across the entire fleet.

208
00:06:56,649 --> 00:06:57,884
Yeah, and in fact, you know,

209
00:06:57,884 --> 00:06:59,652
one of the things we're very excited about is,

210
00:06:59,652 --> 00:07:01,587
given the fleet we now have,

211
00:07:01,587 --> 00:07:03,489
we are able to take advantage

212
00:07:03,489 --> 00:07:05,324
of these software advances

213
00:07:05,324 --> 00:07:06,559
across the entire fleet.

214
00:07:06,559 --> 00:07:07,660
And I think that's one of the things

215
00:07:07,660 --> 00:07:08,795
that people don't understand,

216
00:07:08,795 --> 00:07:10,963
that even the things that were shipped

217
00:07:10,963 --> 00:07:12,498
from you multiple years ago

218
00:07:12,498 --> 00:07:14,567
benefit from software advances.

219
00:07:14,567 --> 00:07:15,067
That's right.

220
00:07:15,067 --> 00:07:16,302
Continuously.

221
00:07:17,270 --> 00:07:18,638
Hopper, we've been shipping

222
00:07:18,638 --> 00:07:20,706
Hoppers to you now for two years.

223
00:07:20,706 --> 00:07:23,776
Over the course of those two years,

224
00:07:23,776 --> 00:07:25,411
through new algorithms,

225
00:07:25,411 --> 00:07:26,579
let's just say Transformer

226
00:07:26,579 --> 00:07:28,448
or let's just say Llama 70B.

227
00:07:28,448 --> 00:07:30,383
Over the course of the last two years

228
00:07:30,383 --> 00:07:33,653
we've improved the performance 40x.

229
00:07:33,653 --> 00:07:35,421
Blackwell to Hoppers 40x,

230
00:07:35,421 --> 00:07:38,658
Hopper on Hopper has been 40x

231
00:07:38,658 --> 00:07:40,026
over the last two years.

232
00:07:40,026 --> 00:07:42,328
We did that through all kinds of things like

233
00:07:42,328 --> 00:07:44,497
in-flight batching,

234
00:07:44,497 --> 00:07:47,733
and speculative decoding,

235
00:07:47,733 --> 00:07:49,635
and all kinds of new algorithms

236
00:07:49,635 --> 00:07:52,538
that sit underneath the inference engine.

237
00:07:52,538 --> 00:07:55,208
And we're willing, of course,

238
00:07:55,208 --> 00:07:56,409
all of the researchers

239
00:07:56,409 --> 00:07:58,411
and all the developers,

240
00:07:58,411 --> 00:07:59,679
they want to invest

241
00:07:59,679 --> 00:08:01,747
in improving the architecture on CUDA

242
00:08:01,747 --> 00:08:02,748
because they know

243
00:08:02,748 --> 00:08:04,584
the install base is so large.

244
00:08:04,584 --> 00:08:06,219
Whatever work that they do

245
00:08:06,219 --> 00:08:08,554
will benefit a lot of the fleet

246
00:08:08,554 --> 00:08:10,923
and many users will be able to benefit from it.

247
00:08:10,923 --> 00:08:13,659
And so, you're willing to dedicate your entire–

248
00:08:13,659 --> 00:08:15,728
you know, all of your heart and soul

249
00:08:15,728 --> 00:08:18,531
into optimizing the algorithm and the architecture

250
00:08:18,531 --> 00:08:20,399
because the install base is so large.

251
00:08:20,700 --> 00:08:22,768
That's so well said because in some sense,

252
00:08:22,768 --> 00:08:25,371
you know, the latest GB200 coming up

253
00:08:25,371 --> 00:08:26,405
means some of the folks

254
00:08:26,405 --> 00:08:27,607
who are doing the cutting edge

255
00:08:27,607 --> 00:08:29,542
new training will go there.

256
00:08:29,542 --> 00:08:30,877
And at the same time,

257
00:08:30,877 --> 00:08:32,678
we are also providing, for example,

258
00:08:32,678 --> 00:08:34,280
in Azure Container Service,

259
00:08:34,280 --> 00:08:35,615
some of the sort of

260
00:08:35,615 --> 00:08:38,618
software gains of the fleet on the GPU fleet

261
00:08:38,618 --> 00:08:40,453
so that they can run any agent on it.

262
00:08:40,453 --> 00:08:42,355
So that ability for us

263
00:08:42,355 --> 00:08:44,123
to be able to flex the fleet

264
00:08:44,123 --> 00:08:47,226
for the various different parts of the frontier

265
00:08:47,226 --> 00:08:49,161
of latency, COGS, and performance

266
00:08:49,161 --> 00:08:50,463
is just fantastic to have.

267
00:08:50,463 --> 00:08:52,031
Yeah, your engineers and ours

268
00:08:52,031 --> 00:08:53,699
are at this moment

269
00:08:53,699 --> 00:08:57,904
optimizing runtimes on Ampere.

270
00:08:57,904 --> 00:09:00,373
A10s, A100s.

271
00:09:00,373 --> 00:09:02,008
And the beautiful thing is,

272
00:09:02,008 --> 00:09:06,345
because of our natural understanding

273
00:09:06,345 --> 00:09:11,584
of the importance of preserving, enhancing

274
00:09:11,584 --> 00:09:15,087
developer productivity and developer value

275
00:09:15,087 --> 00:09:17,189
for the life of the architecture,

276
00:09:17,189 --> 00:09:18,324
we support software and

277
00:09:18,324 --> 00:09:19,592
we fine tune software

278
00:09:19,592 --> 00:09:20,927
for as long as we shall live.

279
00:09:20,927 --> 00:09:21,594
And so,

280
00:09:21,594 --> 00:09:22,929
That’s right. That’s right.

281
00:09:22,929 --> 00:09:25,197
And all of that is just, between our two companies

282
00:09:25,197 --> 00:09:26,899
we’re dedicated to all the developers.

283
00:09:26,899 --> 00:09:28,568
Yeah, I know that's, absolutely.

284
00:09:28,568 --> 00:09:30,469
And then the last one I wanted to unpack

285
00:09:30,469 --> 00:09:32,004
is another thing you talk about,

286
00:09:32,004 --> 00:09:35,308
sort of, GPUs is sort of more broader

287
00:09:35,308 --> 00:09:37,009
than just one AI workload.

288
00:09:37,009 --> 00:09:39,145
It is this accelerated compute vision

289
00:09:39,145 --> 00:09:40,346
you've had, quite frankly,

290
00:09:40,346 --> 00:09:42,481
for, you know, the longest time.

291
00:09:43,082 --> 00:09:44,617
But I think that also is a

292
00:09:44,617 --> 00:09:45,818
massive benefit to us

293
00:09:45,818 --> 00:09:47,453
because that also means we can bring

294
00:09:47,453 --> 00:09:49,555
many, many different classes of workloads.

295
00:09:49,555 --> 00:09:52,658
That means customers can use this fleet,

296
00:09:52,658 --> 00:09:55,394
for, of course, the cutting edge AI,

297
00:09:55,394 --> 00:09:56,395
but many workloads.

298
00:09:56,395 --> 00:09:57,797
But do you want to speak a little bit to the

299
00:09:57,797 --> 00:09:59,765
what happens to utilization

300
00:09:59,765 --> 00:10:01,867
and the diversity of workloads

301
00:10:01,867 --> 00:10:03,669
that get all accelerated?

302
00:10:03,669 --> 00:10:05,371
One of the benefits of CUDA

303
00:10:05,371 --> 00:10:07,139
is that the install base is so large.

304
00:10:07,139 --> 00:10:10,109
Another benefit of CUDA is that,

305
00:10:10,109 --> 00:10:10,943
on the one hand,

306
00:10:10,943 --> 00:10:13,479
it’s an accelerator architecture.

307
00:10:13,479 --> 00:10:14,547
On the other hand,

308
00:10:14,547 --> 00:10:16,582
it’s fairly general-purpose

309
00:10:16,582 --> 00:10:19,552
for some of the heaviest workloads.

310
00:10:19,552 --> 00:10:20,853
Our two teams are working on

311
00:10:20,853 --> 00:10:23,422
accelerating data processing.

312
00:10:23,422 --> 00:10:27,159
Accelerating data processing by 20x, 50x,

313
00:10:27,159 --> 00:10:29,061
transcoding video,

314
00:10:29,061 --> 00:10:32,732
image processing, models of all kinds,

315
00:10:32,732 --> 00:10:34,667
recommender systems,

316
00:10:34,667 --> 00:10:36,969
vector search engines.

317
00:10:36,969 --> 00:10:39,572
All of those different types of algorithms

318
00:10:39,572 --> 00:10:41,974
map wonderfully on top of CUDA.

319
00:10:41,974 --> 00:10:43,542
And so with our computer scientists

320
00:10:43,542 --> 00:10:44,644
working together,

321
00:10:44,644 --> 00:10:45,711
we can accelerate

322
00:10:45,711 --> 00:10:47,580
one workload after another workload,

323
00:10:47,580 --> 00:10:49,081
after another workload.

324
00:10:49,081 --> 00:10:50,416
And so as this tail,

325
00:10:50,416 --> 00:10:51,384
if you will,

326
00:10:51,384 --> 00:10:53,319
the long tail of GPUs

327
00:10:53,319 --> 00:10:55,087
and all of the frontier models

328
00:10:55,087 --> 00:10:57,456
move off to Blackwells and Futures,

329
00:10:57,456 --> 00:11:00,693
we leave behind this compute fleet,

330
00:11:00,693 --> 00:11:02,695
which is still

331
00:11:02,695 --> 00:11:03,763
orders of magnitude

332
00:11:03,763 --> 00:11:05,765
faster than general-purpose computing

333
00:11:05,765 --> 00:11:06,699
that can be used

334
00:11:06,699 --> 00:11:08,467
for all these different workloads.

335
00:11:08,467 --> 00:11:09,802
And so our job

336
00:11:09,802 --> 00:11:11,237
is to make sure that that fleet

337
00:11:11,237 --> 00:11:12,204
and that workload,

338
00:11:12,204 --> 00:11:14,473
all of the workloads of your data center,

339
00:11:14,473 --> 00:11:15,808
are fully accelerated,

340
00:11:15,808 --> 00:11:17,576
the fleet is fully utilized

341
00:11:17,576 --> 00:11:19,578
for the entire life of the fleet.

342
00:11:20,046 --> 00:11:22,782
No that's fantastic and so well captures

343
00:11:22,782 --> 00:11:24,016
that real essence, right?

344
00:11:24,016 --> 00:11:26,185
Ultimately, it's not just tokens

345
00:11:26,185 --> 00:11:27,787
but it's all workloads,

346
00:11:27,787 --> 00:11:29,555
per dollar per watt,

347
00:11:29,555 --> 00:11:32,258
can we really accelerate them all

348
00:11:32,258 --> 00:11:34,627
as we continue to innovate across

349
00:11:34,627 --> 00:11:36,095
both the hardware

350
00:11:36,095 --> 00:11:37,496
and the software boundaries

351
00:11:37,496 --> 00:11:39,265
and really have the compounding effects

352
00:11:39,265 --> 00:11:41,500
ultimately show up for our customers.

353
00:11:41,500 --> 00:11:42,868
So thank you so much Jensen

354
00:11:42,868 --> 00:11:43,669
for the partnership.

355
00:11:43,669 --> 00:11:45,638
Thanks to everybody at NVIDIA

356
00:11:45,638 --> 00:11:48,941
for the amazing work that you all are doing.

357
00:11:48,941 --> 00:11:51,110
And it's great to see these two companies,

358
00:11:51,110 --> 00:11:52,745
that have worked from the very beginning

359
00:11:52,745 --> 00:11:55,147
on many of those architectures,

360
00:11:55,147 --> 00:11:56,282
to come to this point

361
00:11:56,282 --> 00:11:58,250
where there is complete new,

362
00:11:58,250 --> 00:11:59,752
I would say, emergence

363
00:11:59,752 --> 00:12:01,320
of computing that none of us

364
00:12:01,320 --> 00:12:02,321
could have ever imagined,

365
00:12:02,321 --> 00:12:03,255
but is, I think,

366
00:12:03,255 --> 00:12:04,657
going to have a very profound impact.

367
00:12:04,657 --> 00:12:05,725
So thank you again.

368
00:12:05,725 --> 00:12:06,892
And Satya,

369
00:12:06,892 --> 00:12:08,227
thank you for your partnership

370
00:12:08,227 --> 00:12:08,861
and leadership,

371
00:12:08,861 --> 00:12:10,830
and the alignment

372
00:12:10,830 --> 00:12:12,398
between our two organizations

373
00:12:12,398 --> 00:12:14,667
to build the most advanced

374
00:12:14,667 --> 00:12:16,001
infrastructure in the world.

375
00:12:16,001 --> 00:12:17,136
The most advanced

376
00:12:17,136 --> 00:12:18,871
AI factories in the world,

377
00:12:18,871 --> 00:12:20,406
is really a delight.

378
00:12:20,406 --> 00:12:21,974
And here we are,

379
00:12:21,974 --> 00:12:23,676
in just two years time,

380
00:12:23,676 --> 00:12:26,378
AI has transformed so profoundly

381
00:12:26,378 --> 00:12:27,613
and it's just so exciting.

382
00:12:27,613 --> 00:12:28,948
This is the best of times

383
00:12:28,948 --> 00:12:30,549
and the best of times are yet to come.

384
00:12:30,549 --> 00:12:32,284
Thank you so much.